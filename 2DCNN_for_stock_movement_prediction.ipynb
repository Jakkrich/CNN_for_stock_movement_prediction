{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2DCNN_for_stock_movement_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqHh9R_ol7ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive', force_remount=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx7zI1JjoSGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data\n",
        "!wget https://github.com/jetanaso/datasets/raw/master/S50IF_CON.xls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAcJlodzoSIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import necessary libraries\n",
        "!pip install mpl_finance\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mpl_dates\n",
        "import matplotlib\n",
        "from mpl_finance import candlestick_ohlc\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "register_matplotlib_converters()\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras import applications, Input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers, regularizers\n",
        "from keras.models import load_model, Sequential, Model\n",
        "from keras.layers import Dropout, Flatten, Dense, Activation, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow import set_random_seed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHaDtmVwoSK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data preparation\n",
        "cols = ['Date','Open','High','Low','Close','Volume']\n",
        "df = pd.read_csv('S50IF_CON.xls', names=cols, index_col=False, skiprows=1, delimiter='\\t')\n",
        "df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
        "df['Date'] = df['Date'].apply(mpl_dates.date2num)\n",
        "df['Open'] = df['Open'].str.replace(',','').astype('float')\n",
        "df['High'] = df['High'].str.replace(',','').astype('float')\n",
        "df['Low'] = df['Low'].str.replace(',','').astype('float')\n",
        "df['Close'] = df['Close'].str.replace(',','').astype('float')\n",
        "df['Volume'] = df['Volume'].str.replace(',','').astype('float')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG7JWl-yoSNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_steps = 88\n",
        "forecast_horizon = 22 \n",
        "\n",
        "dataset = df.copy()\n",
        "dataset['Up/Down'] = np.where(dataset['Close'].shift(-forecast_horizon) > dataset['Close'], 1, 0)\n",
        "dataset = dataset.iloc[:-forecast_horizon]\n",
        "print(dataset.tail(10))\n",
        "print(dataset.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSrO9nBloSWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split train/validate/test datasets (90:5:5)\n",
        "train_dataset = dataset.iloc[:int(len(dataset.index)*.9),:]\n",
        "validate_dataset = dataset.iloc[int(len(dataset.index)*.9)-n_steps:int(len(dataset.index)*.95),:]\n",
        "test_dataset = dataset.iloc[int(len(dataset.index)*.95)-n_steps:,:]\n",
        "\n",
        "validate_dataset.reset_index(drop=True, inplace=True)\n",
        "test_dataset.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(train_dataset.shape)\n",
        "print(validate_dataset.shape)\n",
        "print(test_dataset.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdpDshX_oSDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare chart pictures (2D) as inputs \n",
        "def chart_to_png(dataset, n_pics='default', window_size=100, train=True, validate=False, test=False):\n",
        "  if n_pics=='default':\n",
        "    n_pics = int(len(dataset)-window_size+1)\n",
        "  else:\n",
        "    n_pics = int(n_pics)\n",
        "  \n",
        "  for i in range(n_pics):\n",
        "    prep_df = dataset.iloc[i:window_size+i,:]\n",
        "    prep_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    fig = plt.figure(figsize=(3,3), facecolor='w')\n",
        "\n",
        "    ax1 = plt.subplot2grid((5,5), (0,0), rowspan=4, colspan=4, facecolor='w')\n",
        "    candlestick_ohlc(ax1, prep_df.iloc[:,:-1].values, width=.6, colorup='g', colordown='r')\n",
        "    pad = 0.1\n",
        "    yl = ax1.get_ylim()\n",
        "    ax1.set_ylim(yl[0]-(yl[1]-yl[0])*pad,yl[1])\n",
        "    ax1.grid(False)\n",
        "    ax1.axis('off')\n",
        "\n",
        "    ax1v = ax1.twinx()\n",
        "    ax1v.bar(prep_df['Date'].values, prep_df['Volume'].values, color='blue')\n",
        "    #ax1v.plot(prep_df['Date'].values, prep_df['Volume'].values, color='#00ffe8', lw=.8)\n",
        "    #ax1v.fill_between(prep_df['Date'].values, prep_df['Volume'].min(), prep_df['Volume'].values, facecolor='#00ffe8', alpha=.5)\n",
        "    ax1v.set_ylim(0, 12*prep_df['Volume'].max())\n",
        "    ax1v.grid(False)\n",
        "    ax1v.axis('off')\n",
        "\n",
        "    plt.show(block=True)\n",
        "\n",
        "    if train==True:\n",
        "      path1 = '/drive/My Drive/Colab Notebooks/chart/train/'\n",
        "    elif validate==True:\n",
        "      path1 = '/drive/My Drive/Colab Notebooks/chart/validate/'\n",
        "    elif test==True:\n",
        "      path1 = '/drive/My Drive/Colab Notebooks/chart/test/'\n",
        "\n",
        "    if (prep_df.iloc[-1,-1]==1) & (test==False):\n",
        "      path2 = path1+'up/'\n",
        "    elif (prep_df.iloc[-1,-1]==0) & (test==False):\n",
        "      path2 = path1+'down/'\n",
        "    elif test==True:\n",
        "      path2 = path1+'all_classes/'\n",
        "\n",
        "    fig.savefig(path2+'chart'+str('{0:04d}'.format(i)), facecolor=fig.get_facecolor())\n",
        "    rgba = Image.open(path2+'chart'+str('{0:04d}'.format(i))+'.png')\n",
        "    rgb = rgba.convert('RGB')\n",
        "    print(np.array(rgb).shape)\n",
        "    im = Image.fromarray(np.array(rgb))\n",
        "    im.save(path2+'chart'+str('{0:04d}'.format(i))+'.png')\n",
        "    rgba.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # Generate pictures from train dataset\n",
        "  chart_to_png(train_dataset, n_pics='default', window_size=int(n_steps), train=True, validate=False, test=False) \n",
        "   # Generate pictures from validate dataset\n",
        "  chart_to_png(validate_dataset, n_pics='default', window_size=int(n_steps), train=False, validate=True, test=False) \n",
        "   # Generate pictures from test dataset\n",
        "  chart_to_png(test_dataset, n_pics='default', window_size=int(n_steps), train=False, validate=False, test=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OGRsWDCoSAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define model architecture\n",
        "img_width, img_height = 216, 216\n",
        "train_data_dir = '/drive/My Drive/Colab Notebooks/chart/train/'\n",
        "validation_data_dir = '/drive/My Drive/Colab Notebooks/chart/validate/'\n",
        "test_data_dir = '/drive/My Drive/Colab Notebooks/chart/test/'\n",
        "epochs = 20\n",
        "batch_size = 16\n",
        "num_models = 30\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def fit_model(train_data_dir, validation_data_dir, epochs, batch_size):\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(16, (11, 11), padding='same', kernel_regularizer=regularizers.l2(0.001), input_shape=(img_width, img_height, 3)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2D(32, (11, 11), activity_regularizer=regularizers.l2(0.001)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2D(128, (11, 11), activity_regularizer=regularizers.l2(0.001)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1, activation='sigmoid')) \n",
        "\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer=optimizers.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, amsgrad=False),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  #model.summary()\n",
        "\n",
        "  # prepare data augmentation configuration\n",
        "  train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                    shear_range=0.2,\n",
        "                                    zoom_range=0.2,\n",
        "                                    horizontal_flip=False,\n",
        "                                    vertical_flip=True,\n",
        "                                    validation_split=0.2)\n",
        "\n",
        "  validate_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "  train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
        "                                                      target_size=(img_height, img_width),\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      class_mode='binary',\n",
        "                                                      shuffle=True,\n",
        "                                                      seed=1)\n",
        "\n",
        "  validation_generator = validate_datagen.flow_from_directory(validation_data_dir,\n",
        "                                                              target_size=(img_height, img_width),\n",
        "                                                              batch_size=1,\n",
        "                                                              class_mode='binary',\n",
        "                                                              shuffle=False)\n",
        "\n",
        "  history = model.fit_generator(train_generator,\n",
        "                                steps_per_epoch=train_generator.n // train_generator.batch_size,\n",
        "                                epochs=epochs,\n",
        "                                verbose=0,\n",
        "                                validation_data=validation_generator,\n",
        "                                validation_steps=validation_generator.n // validation_generator.batch_size)\n",
        "  \n",
        "  # plotting learning curve\n",
        "  plt.plot(history.history['loss'], label='train')\n",
        "  plt.plot(history.history['val_loss'], label='validate')\n",
        "  plt.title('Loss Development')\n",
        "  plt.xticks(np.arange(0,epochs+1,step=2))\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(history.history['acc'], label='train')\n",
        "  plt.plot(history.history['val_acc'], label='validate')\n",
        "  plt.title('Accuracy Development')\n",
        "  plt.xticks(np.arange(0,epochs+1,step=2))\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  return model\n",
        "\n",
        "################################################################################\n",
        "\n",
        "for i in range(num_models):\n",
        "\t# fit model\n",
        "\tmodel = fit_model(train_data_dir, validation_data_dir, epochs, batch_size)\n",
        "\t# save model\n",
        "\tfilename = '/drive/My Drive/Colab Notebooks/2dcnn_model_' + str(i+1) + '.h5'\n",
        "\tmodel.save(filename)\n",
        "\tprint('>Saved %s' % filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxzvZ7Ee1UTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_all_models(n_models):\n",
        "\tall_models = list()\n",
        "\tfor i in range(n_models):\n",
        "\t\tfilename = '/drive/My Drive/Colab Notebooks/2dcnn_model_' + str(i+1) + '.h5'\n",
        "\t\tmodel = load_model(filename)\n",
        "\t\tall_models.append(model)\n",
        "\t\tprint('>loaded %s' % filename)\n",
        "\treturn all_models\n",
        "\n",
        "################################################################################\n",
        "\n",
        "# load all models at once\n",
        "members = load_all_models(num_models)\n",
        "print('Loaded %d models' % len(members))\n",
        "print('='*55)\n",
        "\n",
        "# evaluate standalone models on test dataset\n",
        "acc_list = []\n",
        "for i,model in enumerate(members):  \n",
        "  # make prediction\n",
        "  test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "  test_generator = test_datagen.flow_from_directory(test_data_dir,\n",
        "                                                    target_size=(img_height, img_width),\n",
        "                                                    batch_size=1,\n",
        "                                                    class_mode='binary',\n",
        "                                                    shuffle=False)\n",
        "\n",
        "  pred = model.predict_generator(test_generator, steps=len(test_generator), verbose=1)\n",
        "\n",
        "  # get filenames (set shuffle=false in generator is important)\n",
        "  filenames = test_generator.filenames\n",
        "\n",
        "  # data frame\n",
        "  results = pd.DataFrame({'file':filenames, 'prediction':pred[:,0], 'y_hat':np.round(pred)[:,0]})\n",
        "\n",
        "  # prediction\n",
        "  results['y_true'] = test_dataset['Up/Down'][n_steps-1:].values\n",
        "  acc = accuracy_score(results['y_true'], results['y_hat'])\n",
        "  acc_list.append(acc)\n",
        "  print('Model#%s Accuracy on test dataset: %.3f%%' % (i+1, acc*100))\n",
        "print('='*55)\n",
        "print('Average 2DCNN Accuracy on test dataset: %.3f%%' % (np.mean(acc_list)*100))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}